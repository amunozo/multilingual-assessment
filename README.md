## Assessment of Pre-Trained Models Across Languages and Grammars
This is the code used for the paper [Assessment of Pre-Trained Models Across Languages and Grammars](https://aclanthology.org/2023.ijcnlp-main.23/) by [Alberto Muñoz-Ortiz](amunozo.github.io), [David Vilares](http://www.grupolys.org/~david.vilares/) and [Carlos Gómez-Rodríguez](http://www.grupolys.org/~cgomezr/), presented at the IJCNLP-AACL 2023 at Nusa Dua, Bali, Indonesia.

### Contact

If you have any questions or encounter issues, please feel free to contact the main author at [alberto.munoz.ortiz@udc.es](mailto:alberto.munoz.ortiz@udc.es).

### Acknowledgments

We gratefully acknowledge the support of the following organizations for their contributions to this research:

- **European Research Council (ERC)**: Funding under the Horizon Europe research and innovation programme (SALSA, grant agreement No 101100615).
- **ERDF/MICINN-AEI**: Grant SCANNER-UDC (PID2020-113230RB-C21).
- **Xunta de Galicia**: Grant ED431C 2020/11 and the Centro de Investigación de Galicia “CITIC”, through the collaboration agreement between the Consellería de Cultura, Educación, Formación Profesional e Universidades and the Galician universities for the reinforcement of the research centres of the Galician University System (CIGUS).
- **MCIN/AEI/10.13039/501100011033**: Funding for the FPI 2021 grant (PID2020-113230RB-C21).
